{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "793c1ab0-fb48-4d47-86d7-591e1ab80c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import module\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826fb22-9e1b-4ea8-ba07-59e9899267d9",
   "metadata": {},
   "source": [
    "### Initializing CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e70cad24-0f16-4bef-96c3-6c0d7875abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential() # Empty Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9611c79d-3383-49f1-9eea-968b0eb122fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32, (3, 3), input_shape = (128, 128, 3), activation = \"relu\")) # 1st Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62b58666-7ccc-46b0-b010-eb7fb1d0ad42",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2, 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe7d7a8a-c373-42cf-908b-7c7b6c63f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(64, (3, 3), activation = \"relu\"))# 2nd Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc443535-ba9a-4263-9adb-d3afaac2b34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2, 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ffca2749-ddcc-4679-b7b8-a882d34fde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(128, (3, 3), activation = \"relu\"))# 3rd Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31d594ab-394d-40bb-bf9b-bb67d3f2e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2, 2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30cb7222-7b82-467a-a76c-13aa2d39ec1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten()) # 1d Array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08627628-f287-4294-9655-c6f7e7c9f65f",
   "metadata": {},
   "source": [
    "### Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5feb1ce-6e9d-4848-804a-77ac41ed6a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "187b113f-f87e-4838-8f59-dff498af8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30d42e62-3006-45cc-94c5-34b3a4433d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer='adam',\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97706ae-7e28-4885-8d95-32ca65185c1a",
   "metadata": {},
   "source": [
    "### Fitting Images with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2c2f817-3fd0-4a31-b594-01a0ae9d9bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, \n",
    "                                  shear_range = 0.2,\n",
    "                                  zoom_range = 0.2, \n",
    "                                  horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36c23cfc-c076-4502-bae2-c96e1f0150a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6acdf14a-352b-493f-a206-ed2076deb467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2400 images belonging to 3 classes.\n",
      "Found 600 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "training_dataset = train_datagen.flow_from_directory(\"dataset/training_set\", target_size = (128, 128), batch_size=32, class_mode = \"categorical\")\n",
    "test_dataset = test_datagen.flow_from_directory(\"dataset/test_set\",target_size = (128, 128), batch_size =32, class_mode = \"categorical\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6fa2073c-dad5-40fe-a04c-c016837c4fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "25/25 [==============================] - 18s 724ms/step - loss: 1.2320 - accuracy: 0.3288 - val_loss: 1.0936 - val_accuracy: 0.4018\n",
      "Epoch 2/25\n",
      "25/25 [==============================] - 17s 686ms/step - loss: 1.0861 - accuracy: 0.3963 - val_loss: 1.0536 - val_accuracy: 0.4732\n",
      "Epoch 3/25\n",
      "25/25 [==============================] - 17s 684ms/step - loss: 1.0311 - accuracy: 0.4588 - val_loss: 0.9744 - val_accuracy: 0.5089\n",
      "Epoch 4/25\n",
      "25/25 [==============================] - 17s 662ms/step - loss: 0.9817 - accuracy: 0.5000 - val_loss: 0.9088 - val_accuracy: 0.5223\n",
      "Epoch 5/25\n",
      "25/25 [==============================] - 17s 662ms/step - loss: 0.9186 - accuracy: 0.5275 - val_loss: 0.8545 - val_accuracy: 0.5402\n",
      "Epoch 6/25\n",
      "25/25 [==============================] - 17s 669ms/step - loss: 0.9536 - accuracy: 0.4963 - val_loss: 0.8619 - val_accuracy: 0.6205\n",
      "Epoch 7/25\n",
      "25/25 [==============================] - 17s 672ms/step - loss: 0.8663 - accuracy: 0.5875 - val_loss: 0.7988 - val_accuracy: 0.6205\n",
      "Epoch 8/25\n",
      "25/25 [==============================] - 17s 660ms/step - loss: 0.8493 - accuracy: 0.5800 - val_loss: 0.7636 - val_accuracy: 0.6875\n",
      "Epoch 9/25\n",
      "25/25 [==============================] - 17s 660ms/step - loss: 0.7918 - accuracy: 0.6187 - val_loss: 0.6673 - val_accuracy: 0.7098\n",
      "Epoch 10/25\n",
      "25/25 [==============================] - 17s 679ms/step - loss: 0.7737 - accuracy: 0.6275 - val_loss: 0.8912 - val_accuracy: 0.5893\n",
      "Epoch 11/25\n",
      "25/25 [==============================] - 16s 655ms/step - loss: 0.6879 - accuracy: 0.6762 - val_loss: 0.8090 - val_accuracy: 0.6518\n",
      "Epoch 12/25\n",
      "25/25 [==============================] - 17s 660ms/step - loss: 0.7536 - accuracy: 0.6725 - val_loss: 0.6222 - val_accuracy: 0.7455\n",
      "Epoch 13/25\n",
      "25/25 [==============================] - 16s 651ms/step - loss: 0.7124 - accuracy: 0.6687 - val_loss: 0.6610 - val_accuracy: 0.7143\n",
      "Epoch 14/25\n",
      "25/25 [==============================] - 17s 693ms/step - loss: 0.6818 - accuracy: 0.7000 - val_loss: 0.6364 - val_accuracy: 0.7545\n",
      "Epoch 15/25\n",
      "25/25 [==============================] - 17s 686ms/step - loss: 0.7070 - accuracy: 0.6837 - val_loss: 0.7017 - val_accuracy: 0.6875\n",
      "Epoch 16/25\n",
      "25/25 [==============================] - 17s 681ms/step - loss: 0.7061 - accuracy: 0.6875 - val_loss: 0.6656 - val_accuracy: 0.7321\n",
      "Epoch 17/25\n",
      "25/25 [==============================] - 17s 691ms/step - loss: 0.6154 - accuracy: 0.7375 - val_loss: 0.7232 - val_accuracy: 0.6696\n",
      "Epoch 18/25\n",
      "25/25 [==============================] - 16s 653ms/step - loss: 0.6496 - accuracy: 0.7188 - val_loss: 0.7300 - val_accuracy: 0.6786\n",
      "Epoch 19/25\n",
      "25/25 [==============================] - 17s 661ms/step - loss: 0.5923 - accuracy: 0.7450 - val_loss: 0.6834 - val_accuracy: 0.6920\n",
      "Epoch 20/25\n",
      "25/25 [==============================] - 17s 661ms/step - loss: 0.6094 - accuracy: 0.7375 - val_loss: 0.7118 - val_accuracy: 0.7188\n",
      "Epoch 21/25\n",
      "25/25 [==============================] - 17s 684ms/step - loss: 0.6747 - accuracy: 0.7013 - val_loss: 0.5613 - val_accuracy: 0.7723\n",
      "Epoch 22/25\n",
      "25/25 [==============================] - 17s 663ms/step - loss: 0.5355 - accuracy: 0.7538 - val_loss: 0.5343 - val_accuracy: 0.8036\n",
      "Epoch 23/25\n",
      "25/25 [==============================] - 17s 654ms/step - loss: 0.5059 - accuracy: 0.7850 - val_loss: 0.6559 - val_accuracy: 0.7098\n",
      "Epoch 24/25\n",
      "25/25 [==============================] - 17s 676ms/step - loss: 0.5475 - accuracy: 0.7575 - val_loss: 0.5213 - val_accuracy: 0.7991\n",
      "Epoch 25/25\n",
      "25/25 [==============================] - 17s 666ms/step - loss: 0.5967 - accuracy: 0.7575 - val_loss: 0.5983 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2d735089b20>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(training_dataset, steps_per_epoch=800/32, epochs=25, validation_data=test_dataset, validation_steps= 200/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4208059a-2a0d-45bc-a1b5-d36f39255818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "target_dirctory = \"dataset/model/\"\n",
    "if not os.path.exists(target_dirctory):\n",
    "    os.mkdir(target_dirctory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a56a3154-485c-4352-a3cc-c1d5c467349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"dataset/model/model.h5\")\n",
    "classifier.save_weights(\"dataset/model/weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5f95e578-53cf-422d-8eee-70369c69deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from PIL import Image, ImageTk\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from tkinter import Tk, Label,Canvas, NW, Entry, Button\n",
    "from keras.preprocessing import image as image_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "755bbd74-7d68-498d-a571-1e1fbd5e9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Model\n",
    "image_width, image_height = 128, 128\n",
    "model_path = \"dataset/model/model.h5\"\n",
    "model_wieght_path = \"dataset/model/weights.h5\"\n",
    "model = load_model(model_path)\n",
    "model.load_weights(model_wieght_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4fb2f525-e937-4ebd-b5b2-d8182957226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = ''\n",
    "window = Tk()\n",
    "window.title(\"Image Classification\")\n",
    "window.geometry(\"800x600\")\n",
    "lbl = Label(window, text = \"Please Enter The Image Url\", font = (\"Halvetica\", 16))\n",
    "lbl.pack()\n",
    "\n",
    "\n",
    "def Enter():\n",
    "    global url\n",
    "    lbl.configure()\n",
    "    url = (User_input.get())\n",
    "    print(url)\n",
    "    \n",
    "    \n",
    "    response = requests.get(url)\n",
    "    test_image = Image.open(BytesIO(response.content))\n",
    "    put_image = test_image.resize((400, 400))\n",
    "    test_image = test_image.resize((128, 128))\n",
    "    \n",
    "    img = ImageTk.PhotoImage(put_image)\n",
    "    pic = Label(image = img)\n",
    "    pic.pack()\n",
    "    pic.image = img\n",
    "    test_image = image_utils.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    \n",
    "    result = model.predict_on_batch(test_image)\n",
    "    \n",
    "    \n",
    "    if result[0][0] == 1:\n",
    "        ans = \"French Fries\"\n",
    "    elif result[0][1] == 1:\n",
    "        ans = \"Pizza\"\n",
    "    elif result[0][2] == 1:\n",
    "        ans =  \"samosa\"\n",
    "        \n",
    "    out = Label(window, text = \"Predicted Results: \" + ans, font = (\"Halvetica\", 16))\n",
    "    \n",
    "    out.pack()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0e83dfb-ffda-4778-9e89-e0a9c165662b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://images.immediate.co.uk/production/volatile/sites/30/2021/03/French-fries-b9e3e0c.jpg\n"
     ]
    }
   ],
   "source": [
    "User_input = Entry(width = 100)\n",
    "User_input.pack()\n",
    "button = Button(window, text = \"Detect the Image\", font = (\"Halvetica\", 16), command = Enter)\n",
    "button.pack()\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0fadd1-fa6b-4def-a4df-0f77ca8a1f28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
